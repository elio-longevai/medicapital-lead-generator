# Search API Implementation Plan

## 1. Guiding Principles & Strategy

- **Website First**: The primary source of truth for company data is always the company's own website. The initial `scrape_and_enrich_companies` step using `crawl4ai` is the most important enrichment phase.
- **Refinement as a Fallback**: The multi-provider search strategy outlined below is a fallback mechanism. It will only be triggered within the refinement loop (`execute_refinement_search`) when the initial website scrape fails to gather all required data points.
- **Quality over Quantity**: The goal is not just to get any result, but the *best possible result* to fill data gaps. Therefore, APIs that provide full-page content are prioritized over those that only provide short snippets.

## 2. API Capabilities & Daily Free Limits

By combining the free tiers of multiple APIs, we can create a robust and cost-effective search solution.

| API         | Free Daily Searches (Approx.) | Key Feature                                         |
|-------------|-------------------------------|-----------------------------------------------------|
| **Serper**      | ~83 (`2,500 / 30`)            | Very fast, low-latency, Google-based results.       |
| **Brave**       | ~66 (`2,000 / 30`)            | Independent index, good quality snippets.           |
| **Tavily**      | ~33 (`1,000 / 30`)            | Returns large, clean snippets optimized for RAG.    |
| **Firecrawl**   | ~16 (`500 / 30`)              | **Searches and scrapes the full page content.**     |
| **Total**       | **~198 per day**              | **A diverse pool of free, high-quality searches.**  |


## 3. Task Breakdown

### Task 1: Quality Analysis & Provider Tiering
- **Task:** For a sample of 20 refinement queries, execute searches using Brave, Serper, Tavily, and Firecrawl.
- **Goal:** Compare the quality of the results for the explicit purpose of filling missing data fields.
- **Metric:** Evaluate Firecrawl's full scraped content against the snippets from other providers. Determine which source is most likely to contain the target information.
- **Outcome:** Define a clear priority order (tier list) for the APIs. The proposed initial tier is: 1. Firecrawl, 2. Tavily, 3. Brave, 4. Serper.

### Task 2: Implement Tiered Search Client
- **Task:** Create a new `MultiProviderSearchClient` in `backend/app/core/clients.py`. This client will orchestrate calls to the different APIs.
- **Logic:**
  - The client will manage instances of `BraveClient`, `SerperClient`, `TavilyClient`, and `FirecrawlClient`.
  - It will contain a `search()` method that attempts to get results based on the defined tier list.
  - It will consult the API rate-limit service (from Task 3) before each call. If an API's limit is reached, it will automatically try the next one in the tier.
  - **Firecrawl Specialization**: When calling Firecrawl, it should use the search + scrape feature to get the full markdown content of the top search results, as this is the highest quality data for our refinement task.

### Task 3: API Key and Rate-Limit Management
- **Task:** Implement a robust system to manage API keys and track daily usage to stay within free tiers.
- **Sub-Tasks:**
  1.  **Settings:** Add `SERPER_API_KEY`, `TAVILY_API_KEY`, and `FIRECOWL_API_KEY` to the `Settings` model in `backend/app/core/settings.py` and the corresponding `.env` file.
  2.  **Database Model:** Create a new `ApiUsage` table in `backend/app/db/models.py` to track `(api_name, date, count)`.
  3.  **Service Logic:** Create a new `ApiUsageService` in `backend/app/services/`. This service will have methods like `can_use_api(api_name: str) -> bool` and `increment_usage(api_name: str)`, encapsulating the database interaction.

### Task 4: Integration with Refinement Loop
- **Task:** Integrate the new `MultiProviderSearchClient` into the graph.
- **Sub-Tasks:**
  1.  Instantiate the new client in `backend/app/core/clients.py`.
  2.  In `backend/app/graph/nodes.py`, modify the `execute_refinement_search` node. Replace the direct call to `brave_client.search_async` with a call to the new `multi_provider_search_client.search`.
  3.  Ensure the `extract_and_merge_missing_info` node can handle the potentially larger text content returned by Firecrawl.